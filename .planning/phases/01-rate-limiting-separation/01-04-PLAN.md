---
phase: 01-rate-limiting-separation
plan: 04
type: execute
wave: 3
depends_on: [01-01, 01-02, 01-03]
files_modified:
  - packages/api/src/routes/nivodaProxy.ts
  - packages/api/src/middleware/nivodaProxyAuth.ts
  - packages/api/src/index.ts
  - infrastructure/terraform/modules/container-apps/main.tf
  - infrastructure/terraform/modules/container-apps/variables.tf
autonomous: false
requirements: [RATE-03, RATE-04]

must_haves:
  truths:
    - "Customer API no longer exposes Nivoda proxy route"
    - "Customer API can scale to 10 replicas"
    - "Ingestion proxy enforces 25 req/s global limit"
    - "API scaling does not affect Nivoda rate limit"
  artifacts:
    - path: "packages/api/src/index.ts"
      provides: "API router without proxy route"
      pattern: "(?!.*nivodaProxy)"
    - path: "infrastructure/terraform/modules/container-apps/variables.tf"
      provides: "API min/max replicas variables"
      contains: "api_max_replicas"
  key_links:
    - from: "azurerm_container_app.api.template"
      to: "api_max_replicas variable"
      via: "max_replicas configuration"
      pattern: "max_replicas.*api_max_replicas"
---

<objective>
Complete rate limiting separation by removing Nivoda proxy route from customer API and verifying scaling independence.

Purpose: Enable customer API to scale horizontally (2-10 replicas) for Shopify traffic without violating Nivoda 25 req/s global limit.

Output: Customer API cleaned up (proxy route removed), API scaling variables configured, smoke test verification checkpoint.
</objective>

<execution_context>
@/Users/shaunnesbitt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shaunnesbitt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-rate-limiting-separation/01-RESEARCH.md
@.planning/phases/01-rate-limiting-separation/01-01-SUMMARY.md
@.planning/phases/01-rate-limiting-separation/01-02-SUMMARY.md
@.planning/phases/01-rate-limiting-separation/01-03-SUMMARY.md

# Files to clean up
@packages/api/src/routes/nivodaProxy.ts
@packages/api/src/middleware/nivodaProxyAuth.ts
@packages/api/src/index.ts
</context>

<tasks>

<task type="auto">
  <name>Remove Nivoda proxy route from customer API</name>
  <files>
packages/api/src/routes/nivodaProxy.ts
packages/api/src/middleware/nivodaProxyAuth.ts
packages/api/src/index.ts
  </files>
  <action>
Clean up customer API by removing extracted proxy components:

1. **DELETE** `packages/api/src/routes/nivodaProxy.ts` - Proxy route now lives in ingestion-proxy service

2. **DELETE** `packages/api/src/middleware/nivodaProxyAuth.ts` - Auth middleware now lives in ingestion-proxy service

3. **Update** `packages/api/src/index.ts`:
   - Remove import for nivodaProxyRoute (if exists)
   - Remove `app.use('/api/v2/internal/nivoda', nivodaProxyRoute)` or similar proxy route mounting
   - Verify no other references to nivodaProxy remain

**Rationale:** Customer API no longer needs to proxy Nivoda requests. Scheduler and worker now route through dedicated ingestion proxy. Customer API only serves search/export/holds/orders endpoints.

**What stays in API:**
- Search/export routes (customer-facing)
- Authentication middleware for customer endpoints
- Cache middleware
- All other business logic routes

**What's removed:**
- Internal Nivoda proxy route
- Internal proxy auth middleware
- Any Nivoda-specific rate limiting in API
  </action>
  <verify>
```bash
# Verify files deleted
! test -f packages/api/src/routes/nivodaProxy.ts
! test -f packages/api/src/middleware/nivodaProxyAuth.ts

# Verify no references in index.ts
! grep -q "nivodaProxy" packages/api/src/index.ts

# API still builds
npm run build -w @diamond/api
npm run typecheck -w @diamond/api
```
  </verify>
  <done>
- nivodaProxy.ts and nivodaProxyAuth.ts files deleted from API
- API index.ts no longer mounts proxy route
- API builds successfully without proxy code
  </done>
</task>

<task type="auto">
  <name>Configure API scaling variables</name>
  <files>
infrastructure/terraform/modules/container-apps/variables.tf
infrastructure/terraform/modules/container-apps/main.tf
  </files>
  <action>
Update API scaling configuration to support horizontal scaling:

**1. Update variables.tf:**

Verify or update api_max_replicas variable:
```hcl
variable "api_max_replicas" {
  description = "Maximum number of API replicas for customer traffic (can scale independently now)"
  type        = number
  default     = 10  # Support Shopify traffic scaling
}

variable "api_min_replicas" {
  description = "Minimum number of API replicas"
  type        = number
  default     = 2  # High availability for customer-facing API
}
```

**2. Verify main.tf:**

Confirm API Container App already uses these variables:
```hcl
resource "azurerm_container_app" "api" {
  template {
    min_replicas = var.api_min_replicas
    max_replicas = var.api_max_replicas
    # ...
  }
}
```

**Rationale:** Now that Nivoda proxy route is removed from API, scaling API replicas no longer multiplies the effective rate limit to Nivoda. Customer API can safely scale 2-10 replicas for traffic handling without violating the 25 req/s global limit enforced by single-replica ingestion proxy.
  </action>
  <verify>
```bash
grep -q "api_max_replicas" infrastructure/terraform/modules/container-apps/variables.tf
grep -q "default     = 10" infrastructure/terraform/modules/container-apps/variables.tf
terraform -chdir=infrastructure/terraform/modules/container-apps validate
```
  </verify>
  <done>
- api_max_replicas variable set to 10
- api_min_replicas variable set to 2
- API Container App references these variables
- Terraform validates successfully
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Phase 1 rate limiting separation complete: ingestion proxy deployed, scheduler/worker routed through proxy, customer API cleaned up and ready to scale.
  </what-built>
  <how-to-verify>
**Verification steps:**

1. **Build verification:**
   ```bash
   npm run build
   docker build -f docker/Dockerfile.ingestion-proxy -t diamond-ingestion-proxy:test .
   ```
   Expected: All builds succeed without errors

2. **Terraform plan verification:**
   ```bash
   cd infrastructure/terraform
   terraform plan
   ```
   Expected output should show:
   - `azurerm_container_app.ingestion_proxy` will be created (single replica, internal ingress)
   - `azurerm_container_app_job.scheduler` will be updated (NIVODA_PROXY_BASE_URL added)
   - `azurerm_container_app.worker` will be updated (NIVODA_PROXY_BASE_URL added)
   - `azurerm_container_app.api` unchanged (or minimal changes unrelated to proxy)

3. **Code structure verification:**
   ```bash
   # Ingestion proxy exists
   ls apps/ingestion-proxy/src/index.ts
   ls apps/ingestion-proxy/src/routes/proxy.ts

   # API proxy route removed
   ! test -f packages/api/src/routes/nivodaProxy.ts

   # Terraform outputs ingestion proxy FQDN
   grep "output \"ingestion_proxy_fqdn\"" infrastructure/terraform/modules/container-apps/outputs.tf
   ```

4. **Manual smoke test (if local environment available):**
   - Start ingestion proxy locally: `npm run dev -w @diamond/ingestion-proxy`
   - Send test request with auth token:
     ```bash
     curl -X POST http://localhost:3000/graphql \
       -H "Content-Type: application/json" \
       -H "x-internal-token: test-token" \
       -d '{"query": "query { __typename }"}'
     ```
   - Expected: 200 response (or 401 if token invalid, proving auth works)
   - Check health endpoint: `curl http://localhost:3000/health`
   - Expected: 200 "OK"

**Success indicators:**
- All builds complete successfully
- Terraform plan shows expected changes (no errors)
- File structure matches phase objectives
- Smoke test confirms proxy responds (if tested locally)

**Failure indicators:**
- Build errors in ingestion-proxy or API
- Terraform plan shows errors or unexpected changes
- Missing files or incorrect structure
- Proxy doesn't respond to test requests

**Phase 1 goals achieved:**
- ✓ RATE-01: Dedicated ingestion proxy deployed as separate Container App
- ✓ RATE-02: Scheduler/worker route through proxy (verified in Terraform)
- ✓ RATE-03: Customer API can scale independently (proxy route removed, max_replicas = 10)
- ✓ RATE-04: Global 25 req/s limit (enforced by single-replica proxy)
- ✓ RATE-05: Health checks configured (TCP probes in Terraform)

**What's NOT verified yet (deferred to deployment):**
- Actual load testing under 10 API replicas (requires Azure deployment)
- Ingestion proxy performance under real Nivoda traffic (requires deployed environment)
- End-to-end scheduler → worker → consolidator flow through proxy (requires full pipeline run)

These verifications will happen after Terraform apply in a deployed environment.
  </how-to-verify>
  <resume-signal>
Type "approved" to confirm verification passed, or describe any issues found during verification.
  </resume-signal>
</task>

</tasks>

<verification>
Overall plan verification:

```bash
# Customer API cleaned up
! test -f packages/api/src/routes/nivodaProxy.ts
! grep -q "nivodaProxy" packages/api/src/index.ts

# API builds successfully
npm run build -w @diamond/api

# Scaling variables configured
grep "api_max_replicas" infrastructure/terraform/modules/container-apps/variables.tf
grep "default     = 10" infrastructure/terraform/modules/container-apps/variables.tf

# Full workspace builds
npm run build

# Terraform validates
terraform -chdir=infrastructure/terraform/modules/container-apps validate
```

Expected output:
- Proxy files removed from API
- API builds successfully
- Scaling variables updated
- All builds succeed
- Terraform validates
</verification>

<success_criteria>
1. Nivoda proxy route removed from customer API
2. API max_replicas configured for 10 replicas
3. API min_replicas configured for 2 replicas
4. All builds succeed (API, ingestion-proxy, workspace)
5. Terraform validates successfully
6. Human verification checkpoint confirms phase objectives met
</success_criteria>

<output>
After completion, create `.planning/phases/01-rate-limiting-separation/01-04-SUMMARY.md`
</output>
